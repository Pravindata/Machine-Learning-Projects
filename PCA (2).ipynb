{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.data\n",
    "target = data.target\n",
    "\n",
    "features.shape,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 64)\n",
      "(719, 64)\n",
      "(1078,)\n",
      "(719,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size = 0.6, random_state = 3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9582753824756607\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I have 64 dimensions. Now I want to bring it down to 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14924092, 0.13473079, 0.1156578 , 0.08302537, 0.06011605,\n",
       "       0.05032772, 0.04284676, 0.03556847, 0.03294891, 0.03039427])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348570621050384"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.318855</td>\n",
       "      <td>-11.635362</td>\n",
       "      <td>1.939991</td>\n",
       "      <td>7.151174</td>\n",
       "      <td>4.404023</td>\n",
       "      <td>-2.603868</td>\n",
       "      <td>-2.461337</td>\n",
       "      <td>-8.468723</td>\n",
       "      <td>3.629070</td>\n",
       "      <td>2.791471</td>\n",
       "      <td>...</td>\n",
       "      <td>7.110030</td>\n",
       "      <td>-4.438825</td>\n",
       "      <td>-1.828433</td>\n",
       "      <td>-0.757695</td>\n",
       "      <td>1.438959</td>\n",
       "      <td>4.454482</td>\n",
       "      <td>-1.276321</td>\n",
       "      <td>2.083400</td>\n",
       "      <td>0.267868</td>\n",
       "      <td>0.568966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.695911</td>\n",
       "      <td>22.685769</td>\n",
       "      <td>-5.663167</td>\n",
       "      <td>7.365351</td>\n",
       "      <td>-13.885851</td>\n",
       "      <td>-2.574629</td>\n",
       "      <td>1.732772</td>\n",
       "      <td>-5.481497</td>\n",
       "      <td>1.976303</td>\n",
       "      <td>-7.355780</td>\n",
       "      <td>...</td>\n",
       "      <td>2.457159</td>\n",
       "      <td>-0.954301</td>\n",
       "      <td>-1.415841</td>\n",
       "      <td>0.391963</td>\n",
       "      <td>-3.604504</td>\n",
       "      <td>1.016518</td>\n",
       "      <td>4.636283</td>\n",
       "      <td>-1.929915</td>\n",
       "      <td>1.493422</td>\n",
       "      <td>1.381454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.740055</td>\n",
       "      <td>-2.004610</td>\n",
       "      <td>12.219417</td>\n",
       "      <td>-3.571423</td>\n",
       "      <td>3.145630</td>\n",
       "      <td>-2.756591</td>\n",
       "      <td>23.526970</td>\n",
       "      <td>-0.177478</td>\n",
       "      <td>7.970112</td>\n",
       "      <td>9.555887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268549</td>\n",
       "      <td>0.823661</td>\n",
       "      <td>4.640146</td>\n",
       "      <td>5.505337</td>\n",
       "      <td>4.367166</td>\n",
       "      <td>1.612273</td>\n",
       "      <td>1.795787</td>\n",
       "      <td>1.457364</td>\n",
       "      <td>2.527378</td>\n",
       "      <td>3.081323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-12.985180</td>\n",
       "      <td>-3.619590</td>\n",
       "      <td>-11.197682</td>\n",
       "      <td>4.711520</td>\n",
       "      <td>-11.907296</td>\n",
       "      <td>-4.687477</td>\n",
       "      <td>6.344927</td>\n",
       "      <td>3.376100</td>\n",
       "      <td>3.467681</td>\n",
       "      <td>7.559031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.964102</td>\n",
       "      <td>-7.807109</td>\n",
       "      <td>5.101832</td>\n",
       "      <td>-0.280877</td>\n",
       "      <td>-5.276997</td>\n",
       "      <td>-2.014715</td>\n",
       "      <td>-3.821066</td>\n",
       "      <td>-2.312865</td>\n",
       "      <td>-1.314306</td>\n",
       "      <td>-3.887605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.151770</td>\n",
       "      <td>16.560268</td>\n",
       "      <td>-11.274928</td>\n",
       "      <td>-5.143807</td>\n",
       "      <td>-5.012255</td>\n",
       "      <td>-3.401613</td>\n",
       "      <td>-1.605641</td>\n",
       "      <td>-6.170687</td>\n",
       "      <td>1.059025</td>\n",
       "      <td>-2.773938</td>\n",
       "      <td>...</td>\n",
       "      <td>1.731705</td>\n",
       "      <td>3.393387</td>\n",
       "      <td>1.775847</td>\n",
       "      <td>2.769492</td>\n",
       "      <td>-1.969731</td>\n",
       "      <td>3.815639</td>\n",
       "      <td>-0.528877</td>\n",
       "      <td>1.756822</td>\n",
       "      <td>1.171316</td>\n",
       "      <td>0.929114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>-19.673531</td>\n",
       "      <td>14.276980</td>\n",
       "      <td>10.148741</td>\n",
       "      <td>4.142808</td>\n",
       "      <td>-7.089020</td>\n",
       "      <td>19.398683</td>\n",
       "      <td>0.259775</td>\n",
       "      <td>-1.799209</td>\n",
       "      <td>2.587305</td>\n",
       "      <td>4.072684</td>\n",
       "      <td>...</td>\n",
       "      <td>1.739330</td>\n",
       "      <td>0.631577</td>\n",
       "      <td>-2.463959</td>\n",
       "      <td>-4.136841</td>\n",
       "      <td>-1.641162</td>\n",
       "      <td>2.242803</td>\n",
       "      <td>-2.017200</td>\n",
       "      <td>1.839647</td>\n",
       "      <td>1.962008</td>\n",
       "      <td>3.195684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>0.534993</td>\n",
       "      <td>4.456689</td>\n",
       "      <td>18.702547</td>\n",
       "      <td>-12.131079</td>\n",
       "      <td>14.495314</td>\n",
       "      <td>1.065941</td>\n",
       "      <td>12.424611</td>\n",
       "      <td>-0.051855</td>\n",
       "      <td>0.595268</td>\n",
       "      <td>-2.550900</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.601354</td>\n",
       "      <td>7.103066</td>\n",
       "      <td>1.381400</td>\n",
       "      <td>4.251437</td>\n",
       "      <td>-3.682244</td>\n",
       "      <td>-1.021008</td>\n",
       "      <td>-4.777179</td>\n",
       "      <td>-3.227214</td>\n",
       "      <td>-5.368365</td>\n",
       "      <td>-0.638413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>-0.207629</td>\n",
       "      <td>20.343921</td>\n",
       "      <td>-2.098389</td>\n",
       "      <td>10.006338</td>\n",
       "      <td>-8.424037</td>\n",
       "      <td>-10.729368</td>\n",
       "      <td>4.584058</td>\n",
       "      <td>4.184213</td>\n",
       "      <td>2.723187</td>\n",
       "      <td>-2.465799</td>\n",
       "      <td>...</td>\n",
       "      <td>1.383717</td>\n",
       "      <td>-1.132481</td>\n",
       "      <td>-1.793929</td>\n",
       "      <td>3.978500</td>\n",
       "      <td>-6.162101</td>\n",
       "      <td>-1.678751</td>\n",
       "      <td>2.837904</td>\n",
       "      <td>-0.493410</td>\n",
       "      <td>1.361574</td>\n",
       "      <td>1.762974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>19.786893</td>\n",
       "      <td>-6.562996</td>\n",
       "      <td>18.799105</td>\n",
       "      <td>3.013200</td>\n",
       "      <td>-14.452695</td>\n",
       "      <td>2.414276</td>\n",
       "      <td>0.328284</td>\n",
       "      <td>-6.139185</td>\n",
       "      <td>-5.739256</td>\n",
       "      <td>-4.518639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876699</td>\n",
       "      <td>1.213908</td>\n",
       "      <td>0.110229</td>\n",
       "      <td>-1.901770</td>\n",
       "      <td>0.282352</td>\n",
       "      <td>-4.879710</td>\n",
       "      <td>-4.059534</td>\n",
       "      <td>2.928009</td>\n",
       "      <td>-0.752857</td>\n",
       "      <td>-4.432581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>-12.923120</td>\n",
       "      <td>-11.635136</td>\n",
       "      <td>14.444239</td>\n",
       "      <td>-2.343625</td>\n",
       "      <td>5.901077</td>\n",
       "      <td>-13.875105</td>\n",
       "      <td>-2.435932</td>\n",
       "      <td>0.960980</td>\n",
       "      <td>4.165294</td>\n",
       "      <td>2.927956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951314</td>\n",
       "      <td>1.518725</td>\n",
       "      <td>0.428786</td>\n",
       "      <td>-3.738992</td>\n",
       "      <td>-0.652122</td>\n",
       "      <td>2.868443</td>\n",
       "      <td>3.114627</td>\n",
       "      <td>0.666919</td>\n",
       "      <td>1.754153</td>\n",
       "      <td>2.457588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1078 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0      8.318855 -11.635362   1.939991   7.151174   4.404023  -2.603868   \n",
       "1      3.695911  22.685769  -5.663167   7.365351 -13.885851  -2.574629   \n",
       "2      8.740055  -2.004610  12.219417  -3.571423   3.145630  -2.756591   \n",
       "3    -12.985180  -3.619590 -11.197682   4.711520 -11.907296  -4.687477   \n",
       "4     11.151770  16.560268 -11.274928  -5.143807  -5.012255  -3.401613   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1073 -19.673531  14.276980  10.148741   4.142808  -7.089020  19.398683   \n",
       "1074   0.534993   4.456689  18.702547 -12.131079  14.495314   1.065941   \n",
       "1075  -0.207629  20.343921  -2.098389  10.006338  -8.424037 -10.729368   \n",
       "1076  19.786893  -6.562996  18.799105   3.013200 -14.452695   2.414276   \n",
       "1077 -12.923120 -11.635136  14.444239  -2.343625   5.901077 -13.875105   \n",
       "\n",
       "             6         7         8         9   ...        18        19  \\\n",
       "0     -2.461337 -8.468723  3.629070  2.791471  ...  7.110030 -4.438825   \n",
       "1      1.732772 -5.481497  1.976303 -7.355780  ...  2.457159 -0.954301   \n",
       "2     23.526970 -0.177478  7.970112  9.555887  ... -0.268549  0.823661   \n",
       "3      6.344927  3.376100  3.467681  7.559031  ... -0.964102 -7.807109   \n",
       "4     -1.605641 -6.170687  1.059025 -2.773938  ...  1.731705  3.393387   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "1073   0.259775 -1.799209  2.587305  4.072684  ...  1.739330  0.631577   \n",
       "1074  12.424611 -0.051855  0.595268 -2.550900  ... -1.601354  7.103066   \n",
       "1075   4.584058  4.184213  2.723187 -2.465799  ...  1.383717 -1.132481   \n",
       "1076   0.328284 -6.139185 -5.739256 -4.518639  ...  0.876699  1.213908   \n",
       "1077  -2.435932  0.960980  4.165294  2.927956  ...  0.951314  1.518725   \n",
       "\n",
       "            20        21        22        23        24        25        26  \\\n",
       "0    -1.828433 -0.757695  1.438959  4.454482 -1.276321  2.083400  0.267868   \n",
       "1    -1.415841  0.391963 -3.604504  1.016518  4.636283 -1.929915  1.493422   \n",
       "2     4.640146  5.505337  4.367166  1.612273  1.795787  1.457364  2.527378   \n",
       "3     5.101832 -0.280877 -5.276997 -2.014715 -3.821066 -2.312865 -1.314306   \n",
       "4     1.775847  2.769492 -1.969731  3.815639 -0.528877  1.756822  1.171316   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1073 -2.463959 -4.136841 -1.641162  2.242803 -2.017200  1.839647  1.962008   \n",
       "1074  1.381400  4.251437 -3.682244 -1.021008 -4.777179 -3.227214 -5.368365   \n",
       "1075 -1.793929  3.978500 -6.162101 -1.678751  2.837904 -0.493410  1.361574   \n",
       "1076  0.110229 -1.901770  0.282352 -4.879710 -4.059534  2.928009 -0.752857   \n",
       "1077  0.428786 -3.738992 -0.652122  2.868443  3.114627  0.666919  1.754153   \n",
       "\n",
       "            27  \n",
       "0     0.568966  \n",
       "1     1.381454  \n",
       "2     3.081323  \n",
       "3    -3.887605  \n",
       "4     0.929114  \n",
       "...        ...  \n",
       "1073  3.195684  \n",
       "1074 -0.638413  \n",
       "1075  1.762974  \n",
       "1076 -4.432581  \n",
       "1077  2.457588  \n",
       "\n",
       "[1078 rows x 28 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1078, 28), (719, 28))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape, X_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1078, 64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.inverse_transform(X_train_transformed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(X_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485396383866481\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK\n",
    "\n",
    "1. Perform scaling on cancer data and do the modelling. Check for difference without scaling and with scaling\n",
    "2. Perform PCA with components such that its capturing atleast 50% of the variance and do the modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
